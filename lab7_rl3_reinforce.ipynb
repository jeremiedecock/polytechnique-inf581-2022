{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Policy Gradient\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2022/main/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2022](https://moodle.polytechnique.fr/course/view.php?id=11954) Lab session #7\n",
    "\n",
    "2019-2022 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2022/blob/main/lab7_rl3_reinforce.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2022/main?filepath=lab7_rl3_reinforce.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2022/blob/main/lab7_rl3_reinforce.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2022/raw/main/lab7_rl3_reinforce.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this notebook requires the following libraries: OpenAI *Gym*, NumPy, Pandas, Seaborn and imageio.\n",
    "\n",
    "You can install them with the following command (the next cells do this for you if you use the Google Colab environment):\n",
    "\n",
    "``\n",
    "pip install gym numpy pandas seaborn imageio nnfigs inf581\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install nnfigs inf581\".split(), stdout=subprocess.PIPE)\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "    from inf581 import *\n",
    "\n",
    "from inf581.lab7 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image   # To display GIF images in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\vs}[1]{\\mathbf{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\mathbf{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\U{V}\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "%\n",
    "\\def\\E{\\mathbb{E}}\n",
    "%\\newcommand{transition}{T(s,a,s')}\n",
    "%\\newcommand{transitionfunc}{\\mathcal{T}^a_{ss'}}\n",
    "\\newcommand{transitionfunc}{P}\n",
    "\\newcommand{transitionfuncinst}{P(\\nextstate|\\state,\\action)}\n",
    "\\newcommand{transitionfuncpi}{\\mathcal{T}^{\\pi_i(s)}_{ss'}}\n",
    "\\newcommand{rewardfunc}{r}\n",
    "\\newcommand{rewardfuncinst}{r(\\state,\\action,\\nextstate)}\n",
    "\\newcommand{rewardfuncpi}{r(s,\\pi_i(s),s')}\n",
    "\\newcommand{statespace}{\\mathcal{S}}\n",
    "\\newcommand{statespaceterm}{\\mathcal{S}^F}\n",
    "\\newcommand{statespacefull}{\\mathcal{S^+}}\n",
    "\\newcommand{actionspace}{\\mathcal{A}}\n",
    "\\newcommand{reward}{R}\n",
    "\\newcommand{statet}{S}\n",
    "\\newcommand{actiont}{A}\n",
    "\\newcommand{newstatet}{S'}\n",
    "\\newcommand{nextstate}{\\state'}\n",
    "\\newcommand{newactiont}{A'}\n",
    "\\newcommand{stepsize}{\\alpha}\n",
    "\\newcommand{discount}{\\gamma}\n",
    "\\newcommand{qtable}{Q}\n",
    "\\newcommand{finalstate}{\\state_F}\n",
    "%\n",
    "\\newcommand{\\vs}[1]{\\boldsymbol{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\boldsymbol{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\vit{Value Iteration}\n",
    "\\def\\pit{Policy Iteration}\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "\\def\\cstateset{\\mathcal{X}} %%%\n",
    "\\def\\x{\\vs{x}}                    % TODO cstate\n",
    "\\def\\cstate{\\vs{x}}               %%%\n",
    "\\def\\policy{\\pi}\n",
    "\\def\\piparam{\\vs{\\theta}}         % TODO pparam\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\caction{\\vs{u}}       % action\n",
    "\\def\\cactionset{\\mathcal{U}} %%%\n",
    "\\def\\decision{\\vs{d}}       % decision\n",
    "\\def\\randvar{\\vs{\\omega}}       %%%\n",
    "\\def\\randset{\\Omega}       %%%\n",
    "\\def\\transition{T}       %%%\n",
    "\\def\\immediatereward{r}    %%%\n",
    "\\def\\strategichorizon{s}    %%% % TODO\n",
    "\\def\\tacticalhorizon{k}    %%%  % TODO\n",
    "\\def\\operationalhorizon{h}    %%%\n",
    "\\def\\constalpha{a}    %%%\n",
    "\\def\\U{V}              % utility function\n",
    "\\def\\valuefunc{V}\n",
    "\\def\\X{\\mathcal{X}}\n",
    "\\def\\meu{Maximum Expected Utility}\n",
    "\\def\\finaltime{T}\n",
    "\\def\\timeindex{t}\n",
    "\\def\\iterationindex{i}\n",
    "\\def\\decisionfunc{d}       % action\n",
    "\\def\\mdp{\\text{MDP}}\n",
    "$\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In a previous lab, we have dealt with reinforcement learning in discrete state and action spaces.\n",
    "To do so, we used methods based on action-value function and, especially, $Q$-function estimation.\n",
    "The $Q$-function was stored in a table and updated with on- or off- policy algorithms (namely SARSA and $Q$-Learning). \n",
    "\n",
    "Yet, these methods do not scale to large state spaces and especially not to the case of continuous state spaces.\n",
    "To address these issues one can either extend value-based methods making use of value-function approximation or directly search in policy spaces.\n",
    "In this lab, we will explore the second solution. \n",
    "\n",
    "More specifically, we will search in a family of parameterized policies $\\pi_\\theta(s, a)$ using a policy gradient method.\n",
    "This method performs gradient ascent in the policy space so that the total return is maximized.\n",
    "We will restrict our work to episodic tasks, *i.e.* tasks that have a starting states and last for a finite and fixed number of steps $H$, called horizon. \n",
    "\n",
    "More formally, we define an optimization criterion that we want to maximize:\n",
    "\n",
    "$$J(\\theta) = \\E_{\\pi_\\theta}\\left[\\sum_{t=1}^H r(s_t,a_t)\\right],$$\n",
    "\n",
    "where $\\E_{\\pi_\\theta}$ means $a \\sim \\pi_\\theta(s,.)$ and $H$ is the horizon of the episode.\n",
    "In other words, we want to maximize the value of the starting state: $V^{\\pi_\\theta}(s)$.\n",
    "The policy gradient theorem tells us that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\E_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (s,a) ~ Q^{\\pi_\\theta}(s,a) \\right],\n",
    "$$\n",
    "With \n",
    "\n",
    "$$Q^\\pi(s,a) = \\E^\\pi \\left[\\sum_{t=1}^H r(s_t,a_t)|s=s_1, a=a_1\\right].$$\n",
    "\n",
    "Policy Gradient theorem is extremely powerful because it says one doesn't need to know the dynamics of the system to compute the gradient if one can compute the $Q$-function of the current policy.\n",
    "By applying the policy and observing the one-step transitions is enough.\n",
    "Using a stochastic gradient ascent and replacing $Q^{\\pi_\\theta}(s_t,a_t)$ by a Monte Carlo estimate $R_t = \\sum_{t'=t}^H r(s_{t'},a_{t'})$ over one single trajectory, we end up with a special case of the REINFORCE algorithm (see Algorithm below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "REINFORCE with Policy Gradient theorem Algorithm\n",
    "---\n",
    "\n",
    "Initialize $\\theta^0$ as random<br>\n",
    "Initialize step-size $\\alpha_0$<br>\n",
    "$n \\leftarrow 0$<br>\n",
    "<b>WHILE</b> no convergence<br>\n",
    "\t$\\quad$ Generate rollout $h_n \\leftarrow \\{s_1^n,a_1^n,r_1^n, \\ldots, s_H^n, a_H^n, r_H^n\\} \\sim \\pi_{\\theta^n}$<br>\n",
    "\t$\\quad$ $PG_\\theta \\leftarrow 0$<br>\n",
    "\t$\\quad$ <b>FOR</b> $t=1$ to $H$<br>\n",
    "\t\t$\\quad\\quad$ $R_t \\leftarrow \\sum_{t'=t}^H r_{t'}^n$<br>\n",
    "\t\t$\\quad\\quad$ $PG_\\theta \\leftarrow PG_\\theta + \\nabla_\\theta \\log \\pi_{\\theta^{n}}(s_t,a_t) ~ R_t$<br>\n",
    "\t$\\quad$ $n \\leftarrow n + 1$ <br>\n",
    "\t$\\quad$ $\\theta^n \\leftarrow \\theta^{n-1} + \\alpha_n PG_\\theta$<br>\n",
    "\t$\\quad$  update $\\alpha_n$ (if step-size scheduling)<br>\n",
    "\n",
    "<b>RETURN</b> $\\theta^n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: by replacing the $Q$-function by a Monte-Carlo estimate, we get rid of the Markov assumption and this algorithm is expected to work even in non-Markovian systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided by OpenAI Gym suite.\n",
    "OpenAI Gym provides controllable environments (https://gym.openai.com/envs/) for research in reinforcement learning.\n",
    "Especially, we will try to solve the CartPole-v0 environment (c.f. https://gym.openai.com/envs/CartPole-v0/ and https://github.com/openai/gym/wiki/CartPole-v0) which offers a continuous state space and discrete action space.\n",
    "The Cart Pole task consists in maintaining a pole in a vertical position by moving a cart on which the pole is attached with a joint.\n",
    "No friction is considered.\n",
    "The task is supposed to be solved if the pole stays up-right (within 15 degrees) for 195 steps in average over 100 episodes while keeping the cart position within reasonable bounds.\n",
    "The state is given by $\\{x,\\frac{\\partial x}{\\partial t},\\omega,\\frac{\\partial \\omega}{\\partial t}\\}$ where $x$ is the position of the cart and $\\omega$ is the angle between the pole and vertical position.\n",
    "There are only two possible actions: $a \\in \\{0, 1\\}$ where $a = 0$ means \"push the cart to the LEFT\" and $a = 1$ means \"push the cart to the RIGHT\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Hands on Cart Pole\n",
    "\n",
    "**Task 1:** read  https://gym.openai.com/envs/CartPole-v0/ and https://github.com/openai/gym/wiki/CartPole-v0 to discover the CartPole environment.\n",
    "\n",
    "**Notice:** A reminder of Gym main concepts is available at https://gym.openai.com/docs/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(\"State space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"State upper bounds:\", env.observation_space.high)\n",
    "print(\"State lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policies (for instance constant actions or randomly drawn actions) to discover the CartPole environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with Policy Gradient based Reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the CartPole environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    # TODO\n",
    "\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex1left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    # TODO\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex1right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the CartPole environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for episode_index in range(5):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(70):\n",
    "        env.render_wrapper.render()\n",
    "\n",
    "        if not done:\n",
    "            print(observation)\n",
    "        else:\n",
    "            print(\"x\", end=\"\")\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex1random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement a sigmoid policy\n",
    "\n",
    "As the number of actions is $2$ (push the cart to the left or push it to the right), one can see the problem of controlling the Cart Pole as a binary classification problem.\n",
    "Binary classification can be easily solved thanks to logistic regression which transforms the classification problem into a regression problem using the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `sigmoid` function defined by:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Complete the `logistic_regression` function that implements the logistic regression. This function returns the probability to draw action 1 (\"push right\") w.r.t the parameter vector $\\theta$ and the input vector $s$ (the 4-dimension state vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_logistic_regression_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(s, theta):\n",
    "    prob_push_right = ... # TODO\n",
    "\n",
    "    return prob_push_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Complete the `draw_action` function that draw an action according to current policy i.e. that select the *RIGHT* action with probability $\\sigma(\\theta^\\top s)$, where $\\theta$ is the parameter vector and $s$ is the 4-dimension state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_action(s, theta):\n",
    "    prob_push_right = logistic_regression(s, theta)\n",
    "\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compute $\\nabla_{\\theta} \\log \\pi_\\theta (s,a)$\n",
    "\n",
    "Verify that, for a sigmoid policy:\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{RIGHT}) = \\pi_\\theta (s, \\text{LEFT}) \\times s$\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{LEFT}) = - \\pi_\\theta (s, \\text{RIGHT}) \\times s$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implement REINFORCE\n",
    "\n",
    "Fill the following cells to implement the REINFORCE algorithm (defined in the introduction of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v0\"\n",
    "\n",
    "# Since the goal is to attain an average return of 195, horizon should be larger than 195 steps (say 300 for instance)\n",
    "EPISODE_DURATION = 300\n",
    "\n",
    "ALPHA_INIT = 0.1\n",
    "SCORE = 195.0\n",
    "NUM_EPISODES = 100\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `play_one_episode` function that plays an episode with the given policy $\\pi_\\theta$ (for fixed horizon $H$) and returns its rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an episode\n",
    "def play_one_episode(env, theta, max_episode_length=EPISODE_DURATION, render=False):\n",
    "    s_t = env.reset()\n",
    "\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_states.append(s_t)\n",
    "\n",
    "    for t in range(max_episode_length):\n",
    "\n",
    "        if render:\n",
    "            env.render_wrapper.render()\n",
    "\n",
    "        a_t = ... # TODO\n",
    "        s_t, r_t, done, info = ... # TODO\n",
    "\n",
    "        episode_states.append(s_t)\n",
    "        episode_actions.append(a_t)\n",
    "        episode_rewards.append(r_t)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode_states, episode_actions, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Implement the `score_on_multiple_episodes` function that test the given policy $\\pi_\\theta$ on `num_episodes` episodes (for fixed horizon $H$) and returns:\n",
    "- `success`: `True` if the agent got an average reward greater or equals to 195 over 100 consecutive trials, `False` otherwise\n",
    "- `num_success`: the number of episodes where the agent got an average reward greater or equals to 195\n",
    "- `average_return`: the average reward on the `num_episodes` episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_on_multiple_episodes(env, theta, score=SCORE, num_episodes=NUM_EPISODES, max_episode_length=EPISODE_DURATION, render=False):\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "    return success, num_success, average_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Implement the `compute_policy_gradient` function that returns Policy Gradient for a given episode: policy gradient = $\\sum_{t=1}^H \\nabla_\\theta \\log \\pi_\\theta(s_t,a,_t) R_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Policy Gradient for a given episode\n",
    "def compute_policy_gradient(episode_states, episode_actions, episode_rewards, theta):\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return PG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4**: Implement the `train` function that updates $\\theta$ parameters with gradient ascent until the agent got an average reward greater or equals to 195 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent got an average reward greater or equals to 195 over 100 consecutive trials\n",
    "def train(env, theta_init, max_episode_length = EPISODE_DURATION, alpha_init = ALPHA_INIT):\n",
    "\n",
    "    theta = theta_init\n",
    "    episode_index = 0\n",
    "    average_returns = []\n",
    "\n",
    "    success, _, R = score_on_multiple_episodes(env, theta)\n",
    "    average_returns.append(R)\n",
    "\n",
    "    # Train until success\n",
    "    while (not success):\n",
    "\n",
    "        # TODO\n",
    "\n",
    "    return theta, episode_index, average_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1234)\n",
    "env = gym.make(ENV_NAME)\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "#env.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = env.observation_space.shape[0]\n",
    "\n",
    "# Init parameters to random\n",
    "theta_init = np.random.randn(1, dim)\n",
    "\n",
    "# Train the agent\n",
    "theta, i, average_returns = train(env, theta_init)\n",
    "\n",
    "print(\"Solved after {} iterations\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_on_multiple_episodes(env, theta, num_episodes=10, render=True)\n",
    "env.render_wrapper.make_gif(\"ex4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the evolution of the average reward w.r.t. PG iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training curve\n",
    "plt.plot(range(len(average_returns)),average_returns)\n",
    "plt.title(\"Average reward on 100 episodes\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: solve more challenging environments using Stable-Baselines\n",
    "\n",
    "Knowing main Reinforcement Learning concepts is important to solve RL problems, but being able to quickly solve problems reusing robust and proven implementations of RL algorithms is important too.\n",
    "\n",
    "Stable-Baselines is a Python library that provide state of the art ready to use RL algorithms.\n",
    "\n",
    "The following notebook provides an example of how to use it with Google Colab: https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2022/blob/main/lab7_rl3_reinforce_baselines.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
